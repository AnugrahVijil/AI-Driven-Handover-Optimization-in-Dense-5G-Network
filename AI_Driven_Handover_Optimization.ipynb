{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDGVNsYuJHwh"
      },
      "outputs": [],
      "source": [
        "# Numerical computation libraries\n",
        "import numpy as np               # For numerical operations and arrays\n",
        "import pandas as pd              # For data handling and analysis\n",
        "import random                    # For random action selection - Epsilon-Greedy Algorithm in Reinforcement Learning\n",
        "\n",
        "\n",
        "# Visualization libraries\n",
        "import matplotlib.pyplot as plt  # Plotting graphs\n",
        "import seaborn as sns            # Statistical visualizations\n",
        "\n",
        "# Ignore non-critical warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "# Machine Learning utilities\n",
        "from sklearn.model_selection import train_test_split  # split the dataset into subsets\n",
        "from sklearn.preprocessing import StandardScaler      # Feature scaling - scales parameter\n",
        "from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score)\n",
        "                                                            #|              #|\n",
        "                #checks the predicted value with the original dataset     #cheks accuracy for rewards(RL)\n",
        "from sklearn.ensemble import RandomForestClassifier   # Baseline ML model - combines the predictions of\n",
        "#multiple individual models—in this case, decision trees—to achieve higher accuracy and stability\n",
        "#Model\t                Why Not Used\n",
        "#Logistic Regression\tAssumes linear decision boundary\n",
        "#Linear SVM\t            Cannot model interactions\n",
        "#Kernel SVM\t            High computation, poor scalability\n",
        "#KNN\t                Sensitive to noise and scaling\n",
        "#Naive Bayes\t        Assumes feature independence\n",
        "#Single Decision Tree\tOverfits easily\n",
        "#XGBoost\t            Powerful but more complex to tune\n",
        "\n",
        "# Reinforcement Learning (Gym) - Reinforcement learning is used because handover optimization is a\n",
        "#sequential control problem where each decision affects future network performance and QoE.\n",
        "#Supervised models only predict outcomes, whereas RL learns optimal actions through interaction with\n",
        "#the environment\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces   # observation space (network state) eg. position, location\n",
        "                               # action space (handover decisions) eg. direction\n",
        "\n",
        "# Deep Learning (PyTorch)\n",
        "import torch                  # implements the Deep Q-Network\n",
        "import torch.nn as nn         # defines neural layers.\n",
        "import torch.optim as optim   # update the weights and biases of your neural network\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the 5G handover dataset from Kaggle input directory\n",
        "df = pd.read_csv(\n",
        "    \"/kaggle/input/5g-handover-optimization-dataset-csv/5G_Handover_Optimization_Dataset.csv\"\n",
        ")\n",
        "\n",
        "# Display first few rows to verify data loading\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "UZfyav-bJOBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display dataset structure and data types\n",
        "df.info()  # Shows data types and non-null counts\n",
        "\n",
        "# Show statistical summary of numerical features\n",
        "df.describe()\n",
        "\n",
        "# Check for missing values in each column\n",
        "df.isnull().sum()\n"
      ],
      "metadata": {
        "id": "hFiiNeMxJTeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#correlation heatmap\n",
        "plt.figure(figsize=(30,20))\n",
        "sns.heatmap(df.corr(), annot=True, cmap=\"coolwarm\")\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()\n",
        "#correlation heatmap used to understand the relationship between different network parameters and\n",
        "#call drop behavior.\n",
        "\n",
        "#Correlation means how strongly two parameters are related. Red color shows strong relation, blue shows\n",
        "#weak relation.\n",
        "\n",
        "#The values range from -1 to +1. Values close to 0 indicate very weak or no relationship.\n",
        "\n",
        "#From the heatmap, we can see that most parameters have correlation values close to zero. This means\n",
        "#no single network parameter alone strongly affects call drop.\n",
        "\n",
        "#Because the relationships are weak, simple rule-based methods are not effective. So we use machine\n",
        "#learning to analyze multiple parameters together to predict call drops."
      ],
      "metadata": {
        "id": "WlLC6FToJWWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert raw columns into a single outcome label\n",
        "def derive_outcome(row):\n",
        "    # Successful handover without ping-pong or drop\n",
        "    if row['HO_Success'] == 1 and row['PingPong_Event'] == 0 and row['Session_Drop'] == 0:\n",
        "        return 2   # Successful handover\n",
        "\n",
        "    # Ping-pong handover detected\n",
        "    elif row['PingPong_Event'] == 1:\n",
        "        return 1   # Ping-pong handover\n",
        "\n",
        "    # All other cases are treated as failures\n",
        "    else:\n",
        "        return 0   # Failure / session drop\n",
        "\n",
        "# Apply function row-wise to create new target column\n",
        "df['outcome_encoded'] = df.apply(derive_outcome, axis=1)\n",
        "\n",
        "# Display class distribution\n",
        "df['outcome_encoded'].value_counts()\n"
      ],
      "metadata": {
        "id": "CwOuqEOlJXR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of input features used for ML and RL\n",
        "features = [\n",
        "    'Serving_RSRP_dBm',            # Signal strength of serving cell\n",
        "    'Neighbor_RSRP_dBm',           # Signal strength of neighbor cell\n",
        "    'Serving_SINR_dB',             # SINR of serving cell\n",
        "    'Neighbor_SINR_dB',            # SINR of neighbor cell\n",
        "    'Serving_Cell_Load_pct',       # Load on serving cell\n",
        "    'Neighbor_Cell_Load_pct',      # Load on neighbor cell\n",
        "    'UE_Speed_kmph',               # User speed\n",
        "    'Handover_Count_Last_30s',     # Recent handover history\n",
        "    'PingPong_Last_60s',           # Recent ping-pong history\n",
        "    'Latency_ms'                   # Session latency\n",
        "]\n",
        "\n",
        "# Feature matrix\n",
        "X = df[features]\n",
        "\n",
        "# Target variable\n",
        "y = df['outcome_encoded']\n"
      ],
      "metadata": {
        "id": "JGtzXAoSJcg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "# Initialize standard scaler (zero mean, unit variance)\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit scaler ONLY on training data\n",
        "scaler.fit(X_train)\n",
        "\n",
        "# Store feature names to preserve column alignment\n",
        "feature_names = X_train.columns.tolist()\n",
        "\n",
        "# Transform both train and test sets\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "HDWxD6REJcmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Random Forest classifier\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=200,     # Number of trees\n",
        "    max_depth=10,         # Limit depth to prevent overfitting\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "rf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = rf.predict(X_test_scaled)\n",
        "\n",
        "# Print accuracy score\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Print precision, recall, F1-score\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "q3rRFZusJcuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Baseline ML – Handover Outcome\")\n",
        "plt.show()\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "uB-ayWpLJjkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HandoverEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Custom Gym environment for 5G handover optimization\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, scaler):\n",
        "        super().__init__()\n",
        "\n",
        "        # Store dataset and scaler\n",
        "        self.data = data.reset_index(drop=True)\n",
        "        self.scaler = scaler\n",
        "\n",
        "        # Pointer to current timestep\n",
        "        self.current_step = 0\n",
        "\n",
        "        # Observation space: normalized feature vector\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-5,\n",
        "            high=5,\n",
        "            shape=(len(features),),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # Action space\n",
        "        # 0: Do nothing\n",
        "        # 1: Tune handover parameters\n",
        "        # 2: Trigger handover\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        # Reset environment at beginning of episode\n",
        "        super().reset(seed=seed)\n",
        "        self.current_step = 0\n",
        "        return self._get_state(), {}\n",
        "\n",
        "    def _get_state(self):\n",
        "        # Extract row as DataFrame to preserve feature names\n",
        "        raw = self.data.loc[self.current_step, features].to_frame().T\n",
        "\n",
        "        # Scale features\n",
        "        scaled = self.scaler.transform(raw)\n",
        "\n",
        "        # Return as float32 numpy array\n",
        "        return scaled[0].astype(np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        # Get ground-truth outcome\n",
        "        outcome = self.data.loc[self.current_step, 'outcome_encoded']\n",
        "\n",
        "        # Reward logic based on agent action\n",
        "        if action == 2:      # Trigger handover\n",
        "            reward = 10 if outcome == 2 else -5 if outcome == 1 else -10\n",
        "        elif action == 1:    # Tune parameters\n",
        "            reward = 2 if outcome != 0 else -2\n",
        "        else:                # Do nothing\n",
        "            reward = 1 if outcome == 2 else -3\n",
        "\n",
        "        # Move to next timestep\n",
        "        self.current_step += 1\n",
        "\n",
        "        # Episode termination condition\n",
        "        terminated = self.current_step >= len(self.data) - 1\n",
        "\n",
        "        # Next state\n",
        "        next_state = (\n",
        "            self._get_state()\n",
        "            if not terminated\n",
        "            else np.zeros(len(features), dtype=np.float32)\n",
        "        )\n",
        "\n",
        "        return next_state, reward, terminated, False, {}\n"
      ],
      "metadata": {
        "id": "yXyhfNKPJoG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "    \"\"\"\n",
        "    Deep Q-Network for handover decision-making\n",
        "    \"\"\"\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super().__init__()\n",
        "\n",
        "        # Fully connected neural network\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_size, 64),  # Input layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),          # Hidden layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, action_size) # Output Q-values\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ],
      "metadata": {
        "id": "4vONPKyoJpIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create environment\n",
        "env = HandoverEnv(df, scaler)\n",
        "\n",
        "# State and action dimensions\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "# Initialize DQN\n",
        "model = DQN(state_size, action_size)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# RL hyperparameters\n",
        "gamma = 0.95\n",
        "epsilon = 1.0\n",
        "epsilon_decay = 0.995\n",
        "epsilon_min = 0.01\n",
        "\n",
        "episodes = 20\n",
        "MAX_STEPS = 300\n"
      ],
      "metadata": {
        "id": "EDtK00mPJsN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reward_history = []\n",
        "\n",
        "for episode in range(episodes):\n",
        "    # Reset environment\n",
        "    state, _ = env.reset()\n",
        "    total_reward = 0\n",
        "\n",
        "    for step in range(MAX_STEPS):\n",
        "        state_tensor = torch.tensor(state).unsqueeze(0)\n",
        "\n",
        "        # ε-greedy action selection\n",
        "        if random.random() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                action = model(state_tensor).argmax().item()\n",
        "\n",
        "        # Take action\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "\n",
        "        # Compute target Q-value\n",
        "        next_state_tensor = torch.tensor(next_state).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            target_q = reward + gamma * model(next_state_tensor).max().item() * (not done)\n",
        "\n",
        "        # Current Q-value\n",
        "        current_q = model(state_tensor)[0, action]\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(current_q, torch.tensor(target_q))\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        state = next_state\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Decay exploration rate\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "    reward_history.append(total_reward)\n",
        "\n",
        "    print(\n",
        "        f\"Episode {episode+1}/{episodes} | \"\n",
        "        f\"Reward: {total_reward:.1f} | \"\n",
        "        f\"Epsilon: {epsilon:.3f}\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "IuykytjBJxC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7,4))\n",
        "plt.plot(reward_history, marker='o')\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "plt.title(\"DQN Training Reward Progress\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DvFMnIlFJyJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Human-readable labels\n",
        "outcome_labels = {\n",
        "    0: 'Failure / Session Drop',\n",
        "    1: 'Ping-Pong',\n",
        "    2: 'Successful Handover'\n",
        "}\n",
        "\n",
        "# Map encoded outcomes to labels\n",
        "df['outcome_name'] = df['outcome_encoded'].map(outcome_labels)\n",
        "\n",
        "# Plot distribution\n",
        "df['outcome_name'].value_counts().plot(kind='bar')\n",
        "plt.title(\"Handover Outcome Distribution\")\n",
        "plt.xlabel(\"Outcome Type\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KtBLG7xsJ1M4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}